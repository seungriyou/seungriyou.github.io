---
title: "[Paper] ArcFace: Additive Angular Margin Loss for Deep Face Recognition"
date: 2023-03-17 19:50:00 +0900
categories: [Deep Learning, Paper Review]
tags: [paper review, face recognition, arcface]
math: true
---

> ğŸ“ Paper: <https://arxiv.org/abs/1801.07698>

<br>

ë‹¹ì‹œ face recognition ë¶„ì•¼ì—ì„œ class separabilityë¥¼ ìµœëŒ€í™” í•˜ê¸° ìœ„í•´ softmax loss functionì— marginsë¥¼ adopt í•˜ëŠ” ë°©ì‹ì´ ì—°êµ¬ë˜ê³  ìˆì—ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **Additive Angular Margin Loss (ArcFace)**ë¥¼ ì œì•ˆí•˜ë©°, label noiseì— ê°•í•œ **sub-center ArcFace**ë„ ì œì•ˆí•œë‹¤. ë˜í•œ, inverse problemì¸ feature vectorë¥¼ face imageë¡œ mapping í•˜ëŠ” ë¬¸ì œì—ë„ ì ìš© ê°€ëŠ¥í•¨ì„ ë³´ì—¬ì¤€ë‹¤.

<br>

## 1. Introduction

### 1.1 Recent Studies

face recognition ê´€ë ¨ ì—°êµ¬ì˜ í° ë‘ ì¤„ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

1. multi-class classifierë¥¼ í•™ìŠµí•œë‹¤. (ex. softmax classificer)
2. embeddingì„ í•™ìŠµí•œë‹¤. (ex. triplet loss)

<br>

í•˜ì§€ë§Œ í•´ë‹¹ ë°©ë²•ë“¤ì˜ ë‹¨ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. **softmax loss**
    - open-set face recognition ë¬¸ì œì—ì„œ discriminative í•˜ì§€ ëª»í•˜ë‹¤.
    - linear transformation matrixì˜ í¬ê¸°ê°€ identitiesì˜ ìˆ˜ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•œë‹¤.
2. **triplet loss**
    - large-scale datasetì— ëŒ€í•´ì„œ tripletsì˜ ê°œìˆ˜ê°€ í­ë°œì ìœ¼ë¡œ ëŠ˜ì–´ë‚œë‹¤.
    - semi-hard sample miningì´ ì–´ë µë‹¤.

<br>

margin benefitì„ ì±„íƒí•˜ë©´ì„œë„ triplet lossì—ì„œì˜ sampling problemì„ í”¼í•˜ê¸° ìœ„í•´ì„œ ìµœê·¼ì—ëŠ” margin penaltyë¥¼ more feasible framework(ex. softmax loss)ì— í†µí•©í•˜ëŠ” ë°©ë²•ì´ ì‹œë„ë˜ê³  ìˆë‹¤.

   - embedding featureì™€ linear transformation matrix ê°„ì˜ multiplication stepì—ì„œ global sample-to-class comparisonsê°€ ì´ë£¨ì–´ì§„ë‹¤.
   - linear transformation matrixì˜ ê° columnì€ íŠ¹ì • classë¥¼ ëŒ€í‘œí•˜ëŠ” class centerë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

<br>

SpherefaceëŠ” angular marginì„ ë„ì…í–ˆìœ¼ë©°, unstable trainingì„ ë³´ì™„í•˜ê¸° ìœ„í•´ hybrid loss(standard softmax loss í¬í•¨)ë¥¼ ì œì•ˆí–ˆë‹¤. í•˜ì§€ë§Œ ì´ëŠ” softmax lossê°€ training processë¥¼ dominate í•˜ê²Œ ëœë‹¤.

<br>

### 1.2 ArcFace

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” **training processë¥¼ stabilize**í•˜ê³  **face recognitionì—ì„œ discriminative powerë¥¼ ê°œì„ **í•˜ê¸° ìœ„í•œ <span class="hl">**Additive Angular Margin loss**</span>ë¥¼ ì œì•ˆí•œë‹¤.

- DCNN featureì™€ last fully connected layer ê°„ì˜ dot product = **cosine distance** (after feature and center normalization)
- arc-cosine functionì„ í†µí•´ current featureì™€ target center ê°„ì˜ angleì„ ê³„ì‚°í•œë‹¤.
- **normalized hypersphere**ì—ì„œì˜ **angle**ê³¼ **arc**ì˜ exact correspondenceë¡œ ì¸í•´ ì§ì ‘ì ìœ¼ë¡œ **geodesic distance margin**ì„ ìµœì í™”í•  ìˆ˜ ìˆë‹¤. â†’ **â€œArcFaceâ€**

<br>

### 1.3 Sub-Center ArcFace

margin-based softmax methodsê°€ í° ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¨ì–´ëƒˆìœ¼ë‚˜, ëª¨ë‘ well-annotated clean datasetì— ëŒ€í•´ì„œ í•™ìŠµë˜ì–´ì•¼ í–ˆë‹¤. ë”°ë¼ì„œ real-worldì˜ massive noisy dataì— ëŒ€í•´ robustnessë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì´ í•„ìš”í–ˆë‹¤. â†’ <span class="hl">**â€œsub-center ArcFaceâ€**</span>

- ëª¨ë“  samplesê°€ ëŒ€ì‘ë˜ëŠ” positive centersì— ê°€ê¹ë„ë¡ í•˜ëŠ” **intra-class constraint**ë¥¼ ë„ì…í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰, ê° class ë§ˆë‹¤ K sub-centersê°€ ì¡´ì¬í•˜ë©°, training sampleì´ ì˜¤ì§ ê·¸ K positive sub-centers ì¤‘ í•˜ë‚˜ì—ë§Œ ê°€ê¹Œìš°ë©´ ëœë‹¤.
- **ArcFace**ì˜ ê²½ìš°, noisy sampleì´ large wrong loss valueë¥¼ ìƒì„±í•˜ì—¬ í•™ìŠµì„ ë°©í•´í•˜ì§€ë§Œ, **sub-center ArcFace**ì˜ ê²½ìš° intra-class constraintë¡œ ì¸í•´ training sampleì´ multiple positive sub-centers ì¤‘ í•˜ë‚˜ì— ê°€ê¹ë„ë¡ ê°•ì œí•œë‹¤.
- **sub-center ArcFace**ì—ì„œ noiseëŠ” non-dominant sub-classë¥¼ í˜•ì„±í•˜ê²Œ ëœë‹¤.
    
    | --- | --- |
    | **one dominant sub-class** | majority clean facesë¥¼ í¬í•¨í•œë‹¤. |
    | **multiple non-dominant sub-classes** | hard, noisy facesë¥¼ í¬í•¨í•œë‹¤. |

- ë”°ë¼ì„œ **non-dominant sub-centersì™€ high-confident noisy samplesë¥¼ drop** í•¨ìœ¼ë¡œì¨ ìë™ìœ¼ë¡œ **large-scale clean training data**ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

<br>

### 1.4 Comparison to Other Methods

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-01.png)

#### Local Comparisons

- **triplet loss**: mini-batch ì•ˆì—ì„œ, Euclidean marginì„ ì´ìš©í•œ local sample-to-sample ë¹„êµ
- **tuplet loss**: mini-batch ë‚´ì˜ ëª¨ë“  negative pairsë¥¼ ì´ìš©í•œ ë¹„êµ

#### Global Comparisons

- **ArcFace**: angular marginì„ ì´ìš©í•œ global sample-to-class ë¹„êµ
- **sub-center ArcFace**: angular marginì„ ì´ìš©í•œ global sample-to-subclass ë¹„êµ

<br>

### 1.5 Inverse Problem

- ArcFaceëŠ” **low-dimensinal latent space**ë¥¼ **highly nonlinear face space**ë¡œ **mapping** í•˜ëŠ” ë¬¸ì œì—ë„ ì ìš©ë  ìˆ˜ ìˆë‹¤.
- pre-trained ArcFace modelì€ ì¶”ê°€ì ì¸ generatorë‚˜ discriminatorì˜ í•™ìŠµ ì—†ì´, **gradient**ì™€ **statistic prior(BN layersì— ì €ì¥ëœ meanê³¼ variance)**ë§Œ ì‚¬ìš©í•´ì„œ identity-preserved & visually reasonableí•œ face imagesë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

<br>

### 1.6 Advantages of ArcFace

1. **intuitive**
    - ì§ì ‘ì ìœ¼ë¡œ geodesic distance marginì„ optimize í•œë‹¤. (normalized hypersphereì—ì„œ angleê³¼ arc ê°„ì˜ exact correspondence ë•ë¶„)
    - additive angular margin lossë¡œ ì¸í•´ intra-class compactnessì™€ itner-class discrepancyë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.
2. **economical**
    - sub-classë¥¼ ë„ì…í•˜ì—¬ massive real-world noisesì— ëŒ€í•´ robustnessë¥¼ ê°œì„ í•  ìˆ˜ ìˆë‹¤.
    - sub-center ArcFaceë¥¼ í†µí•´ ìë™ìœ¼ë¡œ large-scale raw web facesë¥¼ cleaning í•  ìˆ˜ ìˆë‹¤.
3. **easy**
    - ê¸°ì¡´ì˜ ì—¬ëŸ¬ deep learning frameworksë¡œ ì‰½ê²Œ êµ¬í˜„ ê°€ëŠ¥í•˜ë‹¤.
    - stable convergenceë¥¼ ìœ„í•´ ë‹¤ë¥¸ loss functionsì™€ ê²°í•©ë  í•„ìš”ê°€ ì—†ë‹¤.
4. **efficient**
    - training ì‹œ ë¬´ì‹œí• ë§Œí•œ computational complexity ë§Œì„ ì¶”ê°€í•œë‹¤.
5. **effective**
    - face recognition benchmarksì—ì„œ state-of-the-art performanceë¥¼ ë‹¬ì„±í–ˆë‹¤. (large-scale image and video datasets)
6. **engaging**
    - discriminative power ë¿ë§Œ ì•„ë‹ˆë¼ generative powerë„ ê°€ì§„ë‹¤.
    - gradientì™€ BN layerì˜ statistic priorsë¥¼ ì´ìš©í•˜ì—¬ identity-preserved & visually plausible face imagesë¥¼ restore í•  ìˆ˜ ìˆë‹¤.

<br>

---

## 2. Related Work

### 2.1 Face Recognition with Margin Penalty

- **Triplet Loss**
    - Euclidean distance marginì„ ì‚¬ìš©í•œë‹¤.
    - mini-batch ë‚´ì—ì„œ **sample-to-sample comparisons**ë¥¼ ìˆ˜í–‰í•œë‹¤. (local)
    - large-scale datasetì—ì„œëŠ” **combinatorial explosionì´ ë°œìƒ**í•˜ë©°, informative mini-batchë¥¼ ì„ íƒí•˜ê³  representative tripletsë¥¼ ê³ ë¥´ëŠ” **effective sampling strategiesê°€ í•„ìš”**í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.
- **Margin-based softmax methods**
    - feasible frameworkì— margin penaltyë¥¼ í†µí•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, **sample-to-class comparisons**ë¥¼ ìˆ˜í–‰í•œë‹¤. (global)
    - sample-to-class comparisonì´ sample-to-sample comparisonë³´ë‹¤ **efficient & stable** í•˜ë‹¤.
        1. class ê°œìˆ˜ê°€ sample ê°œìˆ˜ë³´ë‹¤ í›¨ì”¬ ì ê¸° ë•Œë¬¸ì´ë‹¤.
        2. ê° classëŠ” onlineìœ¼ë¡œ update ë  ìˆ˜ ìˆëŠ” smoothed center vectorë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

<br>

### 2.2 Face Recognition under Noise

ëŒ€ë¶€ë¶„ì˜ face recognition datasetì€ ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë°›ì€ ë°ì´í„°ë¡œ, massive noisy dataì´ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ì–´ ì™”ë‹¤.

<br>

### 2.3 Face Recognition with Sub-classes

subclass divisionì€ different face modalities(ex. frontal view, side view)ì— ì ìš©í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” face recognitionì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.

<br>

### 2.4 Face Synthesis by Model Inversion

- GANì„ ì¤‘ì‹¬ìœ¼ë¡œ identity-preserving face generation ê´€ë ¨ ì—°êµ¬ê°€ ì§„í–‰ë˜ì–´ ì™”ìœ¼ë‚˜, generatorë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ original dataì— ì ‘ê·¼ì´ í•„ìš”í•˜ê²Œ ë˜ì–´ data privacy ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
- ë”°ë¼ì„œ single CNNì„ í†µí•´ image synthesisë¥¼ ìˆ˜í–‰í•˜ëŠ” model inversionì´ ì—°êµ¬ë˜ì—ˆë‹¤.
- generative face recognition model
    - mean faceì™€ íŠ¹ì • faceê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì¸ Eigenfaceê°€ ìˆë‹¤.
    - ìµœê·¼ì—ëŠ” BN layersì— ì €ì¥ëœ statistics(ex. mean, variance)ë¥¼ ì´ìš©í•œ data-free methodê°€ ì œì•ˆë˜ì—ˆë‹¤.

<br>

---

## 3. Proposed Approach

### 3.1 ArcFace

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-02.png)

#### Modification of Softmax

> ê¸°ì¡´ì˜ softmax lossëŠ” intra-class samples ê°„ì— higher similarity / inter-class samples ê°„ì— diversityë¥¼ enfore í•˜ì§€ ëª»í•œë‹¤.
> 
> 
> $$
> L_1=-\log\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\sum_{j=1}^N e^{W_j^T x_i+b_j}}
> $$

<br>

$$
L_3=-\log \frac{e^{s\cos(\theta_{y_i}+m)}}{e^{s\cos(\theta_{y_i}+m)}+\sum_{j=1, j\ne y_i}^N e^{s\cos\theta_j}}
$$

- logitì„ $$W_j^Tx_i=\|W_j\|\|x_i\|\cos\theta_j$$ ë¡œ ë³€ê²½í•œë‹¤.
    - $$\theta_j$$ = weight $$W_j$$ì™€ feature $$x_i$$ ê°„ì˜ ê°ë„
    - $$\|W_j\|=1$$ë¡œ ê³ ì • (L2 normalization)
    - $$\|x_i\|$$ = L2 normalization â†’ re-scale to $$s$$ (= radius of hypersphere)
    - predictionì´ featureì™€ weight ê°„ì˜ ê°ë„ì—ë§Œ ì˜ì¡´í•˜ë„ë¡ í•œë‹¤.
- embedding feautresê°€ hypersphere ìœ„ì˜ ê° feature center ì£¼ë³€ì— ë¶„í¬í•˜ê²Œ ë˜ë¯€ë¡œ, $$x_i$$ì™€ $$W_{y_i}$$ ì‚¬ì´ì— **additive angular margin penalty** $$m$$ì„ ë¶€ì—¬í•˜ì—¬ intra-class compactnessì™€ inter-class discrepancyë¥¼ í–¥ìƒì‹œí‚¨ë‹¤.
    
    > additive angular margin penalty
    > 
    > == geodesic distance margin penalty in the normalized hypershpere 
    > 
    > â†’ **â€œArcFaceâ€**
    {: .prompt-tip}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-03.png){: style="max-width: 70%"}

#### Margin Penalty ë°©ë²• ê°„ ë¹„êµ

ëª¨ë“  methodë¥¼ í•˜ë‚˜ì˜ í†µí•©ëœ frameworkë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- **SphereFace** = multiplicative angular margin $$m_1$$
- **ArcFace** = additive angular margin $$m_2$$
- **CosFace** = additive cosine margin $$m_3$$

$$
L_4=-\log \frac{e^{s\cos(m_1\theta_{y_i}+m_2)-m_3}}{e^{s\cos(m_1\theta_{y_i}+m_2)-m_3}+\sum_{j=1, j\ne y_i}^N e^{s\cos\theta_j}}
$$

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-04.png){: style="max-width: 70%"}

#### Geometric Difference

- additive angular marginì€ geodesic distanceì™€ ì •í™•íˆ ëŒ€ì‘ë˜ì–´ ë” ë‚˜ì€ geometric attributeë¥¼ ê°€ì§€ê³  ìˆë‹¤.
- binary classification caseì—ì„œì˜ decision boundaries
    - **ArcFace** = constant linear angular margin
    - **SphereFace, CosFace** = nonlinear angular margin
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-05.png){: style="max-width: 80%"}
    

#### Other Intra and Inter Losses

> $$L_2$$
> 
> 
> $$
> L_2=-\log \frac{e^{s\cos\theta_{y_i}}}{e^{s\cos\theta_{y_i}}+\sum_{j=1, j\ne y_i}^N e^{s\cos\theta_j}}
> $$

<br>

- **intra-loss**: sample - ground truth center ê°„ì˜ angle/arcë¥¼ ê°ì†Œì‹œì¼œ intra-class compactnessë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ê²ƒì´ë‹¤.
    
    $$
    L_5=L_2+\frac{1}{\pi}\theta_{y_i}
    $$
    
- **inter-loss**: ë‹¤ë¥¸ centersê°„ angle/arcë¥¼ ì¦ê°€ì‹œì¼œ inter-class discrepancyë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ê²ƒì´ë‹¤.
    
    $$
    L_6=L_2-\frac{1}{\pi(N-1)}\sum_{j=1, j\ne y_i}^N \arccos(W_y^T, W_j)
    $$
    

- ë‹¤ìŒê³¼ ê°™ì´ angular representationì— triplet lossë¥¼ ì ìš©í•œë‹¤.
    
    $$
    \arccos (x_i^{pos}, x_i)+m \le \arccos(x_i^{neg}, x_i) 
    $$
    

<br>

### 3.2 Sub-center ArcFace

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-06.png){: style="max-width: 70%"}


- ê° identity ë§ˆë‹¤ sub-classë¥¼ ì‚¬ìš©í•˜ì—¬ noiseì— robust í•´ì§€ë„ë¡ í•œë‹¤.
    - ê° identityì— ëŒ€í•´ $$K$$ sub-centersë¥¼ ì„¤ì •í•œë‹¤.
    - embedding features: $$\mathbf x_i \in \mathbb R^{512}$$
    - all sub-centers: $$W \in \mathbb R^{512\times N\times K}$$
    - subclass-wise similarity scores: $$\mathcal S \in \mathbb R^{N\times K}$$ (by matrix multiplication $$W^T\mathbf x_i$$)
    - class-wise similarity score (by max pooling $$\mathcal S$$): $$\mathcal S' \in \mathbb R^{N\times 1}$$
- Sub-center ArcFace loss
    
    $$
    L_7 =-\log \frac{e^{s\cos(\theta_{y_i}+m)}}{e^{s\cos(\theta_{y_i}+m)}+\sum_{j=1, j\ne y_i}^N e^{s\cos\theta_j}}
    $$
    
    $$
    \theta_j=\arccos (\max_k(W_{j_k}^T\mathbf x_i)), k\in \{1, ..., K\}
    $$
    

#### Dropping High-Confident Noisy Data

> ìë™ìœ¼ë¡œ facesë¥¼ cluster í•˜ì—¬ hard / noisy samplesê°€ dominant clean samplesë¡œë¶€í„° ë¶„ë¦¬ë˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.
> 
> - ì¼ë¶€ noisy samplesëŠ” ê·¸ë“¤ì˜ centersë¡œë¶€í„° ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆë‹¤.
> - hard threshold(ex. angle $$\ge 77^{\circ}$$ or cosine $$\le 0.225$$)ë¥¼ ì£¼ì–´ noisy tailë¥¼ ì œê±°í•  ìˆ˜ ìˆë‹¤.
{: .prompt-info}

- samplesì˜ 4ê°€ì§€ ë¶„ë¥˜
    - **easy clean samples**: dominant sub-classesë¡œë¶€í„° ë‚˜ì˜¨ samples
    - **hard noisy samples**: dominant sub-classesë¡œë¶€í„° ë‚˜ì˜¨ samples
    - **hard clean samples**: non-dominant sub-classesë¡œë¶€í„° ë‚˜ì˜¨ samples
    - **easy noisy samples**: non-dominant sub-classesë¡œë¶€í„° ë‚˜ì˜¨ samples
- sub-center ArcFaceë¥¼ í†µí•´ noise rateë¥¼ 1/3 ê°€ëŸ‰ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆë‹¤. (38.47% â†’ 12.40%)
- Figure 7(b)ì™€ 7(d)ë¥¼ í†µí•´ high-confident noisy samplesë¥¼ drop í•˜ê¸° ìœ„í•œ **constant angle threshold**($$70^{\circ}$$ ì™€ $$80^{\circ}$$ ì‚¬ì´)ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.
- ì´ì²˜ëŸ¼ high-confident noisy dataë¥¼ dropí•œ ì´í›„, automatically cleaned datasetì—ì„œ ArcFace modelì„ **retrain from scratch** í•œë‹¤.

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-07.png){: style="max-width: 70%"}


<br>

### 3.3 Inversion of ArcFace

pre-trained ArcFace modelì„ í†µí•´ identity preserved & visually plausible face imagesë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ë•Œ, ë‹¤ìŒì˜ ì •ë³´ë§Œì„ ì´ìš©í•œë‹¤.

- ArcFace Lossì˜ gradient
- BN layersì— ì €ì¥ëœ face statistic priors (ex. mean, variance)
    
    $$
    L_8=\sum_{i=0}^L\|\tilde \mu_i^r-\mu_i\|_2+\|\tilde \sigma_i^r-\sigma\|_2^2
    $$
    
    - $$\mu_i^r/\sigma_i^r$$: layer $$i$$ì—ì„œì˜ ë¶„í¬ì— ëŒ€í•œ mean / standard deviation
    - pre-trained ArcFaceì˜ $$i$$-th BN layerì— ê°ê°ì— ëŒ€ì‘ë˜ëŠ” parametersê°€ ì €ì¥ë˜ì–´ ìˆë‹¤.

#### Algorithm

Algorithm 1ì—ì„œì²˜ëŸ¼ $$L_3+\lambda L_8, \lambda=0.05$$ ë¥¼ $$T$$ stepsì— ê±¸ì³ optimize í•œ í›„, pre-defined identityì™€ ë™ì¼í•œ identityì´ë©° original datasetì˜ statistical distributionê³¼ ë¹„ìŠ·í•œ facesë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤.

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-08.png){: style="max-width: 70%"}

#### Open-Set Face Generation
embedding featureë¡œë¶€í„° open-set face generationì„ í•´ê²°í•˜ê¸° ìœ„í•´ feature pairs ê°„ì˜ classification lossë¥¼ L2 lossë¡œ ëŒ€ì²´í•œë‹¤.

> - **open-set face generation**
> 
>   : restore the face image from **any embedding feature**
> 
> - **close-set face generation**
> 
>   : reconstructs face images from the **class centers** stored in the linear weight
{: .prompt-info}


<br>

---

## 4. Experiments

### 4.1 Implementation Details

#### Dataset

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-09.png){: style="max-width: 70%"}

#### Experimental Settings

- **ArcFace**
    - RetinaFaceë¥¼ í†µí•´ normalized face crops (112x112)ë¥¼ ìƒì„±í•œë‹¤.
    - embedding networkë¡œëŠ” ResNet50, ResNet100(w/o bottleneck structure)ì„ ì‚¬ìš©í•˜ë©°, ìµœì¢…ì ìœ¼ë¡œ 512-D embedding featureë¥¼ ì–»ëŠ”ë‹¤.
    - feature scale $$s$$  = 64, angular margin $$m$$ = 0.5
    - ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ ì°¸ê³ 
- **Sub-center ArcFace**
    - first round modelì— ëŒ€í•´ ë‹¤ìŒì„ drop í•œë‹¤.
        - non-dominant sub-centers ($$K=3\downarrow 1$$)
        - high-confident noisy data ($$> 75^{\circ}$$)
    - ê·¸ í›„, cleaned dataë¥¼ ê°€ì§€ê³  retrain from scratch í•œë‹¤.

<br>

### 4.2 Ablation Study on ArcFace

ArcFaceê°€ ëª¨ë“  3ê°œì˜ test setsì—ì„œ highest verification accuracyë¥¼ ë‹¬ì„±í–ˆë‹¤.

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-10.png){: style="max-width: 70%"}

<br>

### 4.3 Ablation Study on Sub-center ArcFace

noisy training datasetì— ëŒ€í•´ sub-center ArcFaceë¥¼ ì ìš©í•´ë³´ì•˜ë‹¤.

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-11.png){: style="max-width: 70%"}

- ArcFaceëŠ” noisy datasetì—ì„œ performance drop(96.50% â†’ 90.27%)ì´ ìˆëŠ” ë°˜ë©´, sub-center ArcFaceëŠ” ë” robust(93.72%) í–ˆë‹¤.
- ë„ˆë¬´ ë§ì€ sub-centers(= too large $$K$$)ëŠ” intra-class compactnessë¥¼ ì•½í™”ì‹œì¼œ accuracyë¥¼ í¬ê²Œ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆë‹¤. noise toleranceì™€ intra-class compactness ê°„ balanceë¥¼ ì˜ ìœ ì§€í•´ì•¼ í•˜ë©°, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $$K=3$$ì„ ì„ íƒí–ˆë‹¤.
- nearest sub-center assignment ì‹œ max poolingì„ ì‚¬ìš©í•  ë•Œê°€ softmax poolingì„ ì‚¬ìš©í•  ë•Œë³´ë‹¤ ì„±ëŠ¥ì´ ì‚´ì§ ë” ì¢‹ì•˜ë‹¤. (93.72% vs. 93.55%)
- non-dominant sub-centersì™€ high-confident noisy samplesë¥¼ drop í•˜ëŠ” ê²ƒì„ í†µí•´ regularizationì„ ì¶”ê°€í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤. (95.92% vs. 93.64%)
- constant thresholdì— í¬ê²Œ sensitive í•˜ì§€ ì•Šìœ¼ë©°(95.91% vs. 95.92% vs. 95.74%), ë³¸ ì‹¤í—˜ì—ì„œëŠ” high-confident noisy samplesë¥¼ drop í•˜ê¸° ìœ„í•œ thresholdë¡œ $$75^{\circ}$$ë¥¼ ì„ íƒí–ˆë‹¤.
- co-mining, re-weighting ë“±ì˜ ë°©ë²•ë“¤ë³´ë‹¤ ArcFaceê°€ automatic clean & noisy data isolationì— ë” íš¨ê³¼ì ì´ë‹¤.
- noisy datasetì— ëŒ€í•´ í•™ìŠµí•œ sub-center ArcFaceëŠ” manually cleaned datasetì— ëŒ€í•´ í•™ìŠµí•œ ArcFaceì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. (95.92% vs. 96.50%)

<br>

### 4.4 Benchmark Results

<details>
<summary>Tables & Graphs</summary>
<div markdown="1">

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-12.png){: style="max-width: 70%"}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-13.png){: style="max-width: 70%"}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-14.png){: style="max-width: 70%"}


![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-15.png){: style="max-width: 70%"}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-16.png){: style="max-width: 70%"}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-17.png){: style="max-width: 70%"}


![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-18.png){: style="max-width: 70%"}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-19.png){: style="max-width: 70%"}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-20.png){: style="max-width: 70%"}

</div>
</details>

<br>

### 4.5 Inversion of ArcFace

#### Close-set Face Generation

ê° identityì— ëŒ€í•´, different random inputsë¥¼ ì…ë ¥ í›„ gradually update í•œ ê²°ê³¼ì´ë‹¤.

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-21.png){: style="max-width: 70%"}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-22.png){: style="max-width: 70%"}

#### Open-set Face Generation

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-23.png){: style="max-width: 70%"}

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2023-03-17-24.png){: style="max-width: 80%"}

<br>

---

## 5. Conclusions

- face recognitionì—ì„œ feature embeddingì— ëŒ€í•´ discriminative powerë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” **Additive Angular Margin Loss function(ArcFace)**ì„ ì œì•ˆí–ˆë‹¤.
- ë˜í•œ, massive real-world noisesì— ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” **sub-class ArcFace**ë¥¼ ì œì•ˆí–ˆë‹¤. ì´ë¥¼ í†µí•´ noisy datasetì— ëŒ€í•´ automatic isolationì´ ê°€ëŠ¥í•˜ë‹¤.
    - **one dominant sub-class** = majority of clean faces
    - **non-dominant sub-classes** = hard or noisy faces
- ArcFaceë¥¼ í†µí•´ feature vectorsë¥¼ face imagesë¡œ mapping í•˜ëŠ” **generative power**ë¥¼ ê°•í™”í•  ìˆ˜ ìˆë‹¤.
    - network gradientì™€ BN priorsë§Œì„ ì´ìš©í•œë‹¤.
    - close-setê³¼ open-set ë¬¸ì œì— ëŒ€í•´ ëª¨ë‘ ì ìš©í•  ìˆ˜ ìˆë‹¤.
