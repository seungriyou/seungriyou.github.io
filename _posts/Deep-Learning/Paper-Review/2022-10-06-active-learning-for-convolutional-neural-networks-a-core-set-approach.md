---
title: "[Paper] Active Learning for Convolutional Neural Networks: A Core-Set Approach"
date: 2022-10-06 19:50:00 +0900
categories: [Deep Learning, Paper Review]
tags: [paper review, active learning, core-set]
math: true
---

> ğŸ“ Paper: <https://arxiv.org/abs/1708.00489>

<br>

active learningì— ëŒ€í•´ ì•Œì•„ë³´ë©° ì½ì—ˆë˜ ë…¼ë¬¸ì´ë‹¤.

<br>

## 1. Introduction

- deep modelì„ í•™ìŠµì‹œí‚¤ë ¤ë©´ ì•„ì£¼ ë§ì€ supervised examplesë¡œ ì´ë£¨ì–´ì§„ datasetì´ í•„ìš”í•˜ë‹¤.
    - í•˜ì§€ë§Œ ì´ë¥¼ ëª¨ìœ¼ëŠ” ê²ƒì€ **time consuming & expensive** í•˜ë‹¤.
    - ë”°ë¼ì„œ labeling í•  ê°€ì¹˜ê°€ ìˆëŠ” imageë¥¼ ê³ ë¥´ëŠ” effective í•œ ë°©ë²•(= **active learning**)ì´ í•„ìš”í•˜ë‹¤.
- í•˜ì§€ë§Œ ê¸°ì¡´ì˜ active learning heuristicsëŠ” **batch settingì˜ CNNì— ì ìš©í•˜ê¸°ì— ì ì ˆí•˜ì§€ ì•Šë‹¤**.
    - **ê¸°ì¡´ì˜ ë°©ë²•**ì€ ê° iterationì—ì„œ single pointë¥¼ ì„ íƒí•œë‹¤.
        - single pointëŠ” local optimization ê¸°ë²•ìœ¼ë¡œ ì¸í•´ CNNì—ì„œ í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ëª»í•œë‹¤.
        - ê° iterationì€ convergenceê¹Œì§€ì˜ full trainingì„ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì— label í•˜ë‚˜í•˜ë‚˜ë¥¼ query í•˜ê¸° ì–´ë µë‹¤.
    - ê° iterationì—ì„œ **large subsetì˜ labelì„ query í•˜ëŠ” ë°©ë²•**ì´ í•„ìš”í•˜ë‹¤.
- ë”°ë¼ì„œ active learningì„ **core-set selection** ë¬¸ì œë¡œ ì •ì˜í•œë‹¤.
    - ëª¨ë“  data pointsë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œ model ë§Œí¼ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë„ë¡ subset of pointsë¥¼ ê³ ë¥´ëŠ” ê²ƒì´ë‹¤.
    - data pointsì˜ geometryë¥¼ ì´ìš©í•˜ì—¬ subsetê³¼ remaining points ê°„ì˜ **average lossì˜ boundë¥¼ minimize** í•œë‹¤. (== k-center problem)
- ì´ëŸ¬í•œ ë°©ë²•ì€ image classification taskì—ì„œ ê¸°ì¡´ì˜ ì ‘ê·¼ë²•ë“¤ê³¼ í° ì°¨ì´ë¡œ outperform í•˜ëŠ” ëª¨ìŠµì„ ë³´ì˜€ë‹¤.

<br>

---

## 2. Related Work

### Active Learning

- ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ **large-scale datasetsì— í™•ì¥ë  ìˆ˜ ì—†ì–´** batch samplingì„ ì‚¬ìš©í•˜ëŠ” large CNNsì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ì—†ì—ˆë‹¤. (empirical analysis)
    - information theoretical methods
    - ensemble approaches
    - uncertainty based methods (entropy, geometric distance to distance boundaries)
    - Bayesian active learning (w/ dropout & approximate Bayesian inference)
- ìµœê·¼ì—ëŠ” **optimization based approaches**ê°€ ë“±ì¥í–ˆë‹¤.
    - uncertaintyì™€ diversityë¥¼ trade-off í•˜ì—¬, batch mode active learning settingì—ì„œ hard examplesì˜ diverse setì„ ì–»ëŠ” ë°©ë²•ì´ë‹¤.
    - í•˜ì§€ë§Œ ë§ì€ ì–‘ì˜ variablesê°€ í•„ìš”í•˜ì—¬ ë§ˆì°¬ê°€ì§€ë¡œ large-scale datasetì— í™•ì¥ë  ìˆ˜ ì—†ëŠ” ê²½ìš°ê°€ ë§ë‹¤.
- ë˜í•œ, **pool based active learning algorithms**ë„ ë“±ì¥í–ˆë‹¤.
    - pool based on uncertainty ì´ë©´ì„œ, query í•  pointë¥¼ ê³ ë¥¼ ë•Œì—ëŠ” diversityë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•ë„ ë“±ì¥í–ˆë‹¤.
- ì¶”ê°€ì ì¸ ê¸°ë²•ì„ í†µí•´ **greedy active learning**ì´ batch settingì—ì„œë„ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì—°êµ¬ë„ ì§„í–‰ë˜ì—ˆë‹¤.
- ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ë°©ë²•ì€ uncertainty informationì„ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šê³  **diversity**ë¥¼ í™œìš©í•˜ëŠ” **discrete** **optimization based method**ì— ì†í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

<br>

### Core-Set Selection

- **ëª¨ë“  data pointsë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œ model** ë§Œí¼ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë„ë¡ **subset of pointsë¥¼ ê³ ë¥´ëŠ” ê²ƒ**ì´ë‹¤.
- unsupervised subset selection algorithmê³¼ ë¹„ìŠ·í•˜ë©°, datasetì˜ diverse coverë¥¼ ì°¾ëŠ” ë¬¸ì œì´ë‹¤.

<br>

### Ladder Network

- ë³¸ ë…¼ë¬¸ì˜ **weakly-supervised learning** ì‹¤í—˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ì´ë‹¤.
- supervised learningê³¼ unsupervised learning ë¬¸ì œë¥¼ ê²°í•©í•´ì„œ í•´ê²°í•˜ëŠ” ë°©ì‹ì´ë‹¤. (labelì´ ìˆëŠ” ë°ì´í„°ì™€ ì—†ëŠ” ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.)
- supervised cost functionê³¼ unsupervised cost functionì„ í•©ì³ ë™ì‹œì— minimize í•œë‹¤.

<br>

### ê¸°ì¡´ ë°©ë²•ë¡ ê³¼ì˜ ì°¨ë³„ì 

- active learning ë¬¸ì œë¥¼ **core-set selection** ë¬¸ì œë¡œ ì •ì˜í–ˆë‹¤.
- **fully-supervised**ì™€ **weakly-supervised** ìƒí™©ì„ ëª¨ë‘ ê³ ë ¤í–ˆë‹¤.
- ë‹¤ë¥¸ ê°€ì • ì—†ì´ core-set selection ë¬¸ì œë¥¼ CNNì— ì§ì ‘ ì ìš©í–ˆë‹¤.

<br>

---

## 3. Problem Definition

- ì°¸ê³  ìë£Œ
    - [Have A Nice AI](https://kmhana.tistory.com/6)
    - [PR-119 "Active Learning For Convolutional Neural Networks: A Core-Set Approach" Review (2018 ICLR)](https://aigong.tistory.com/330)
    
- **notation**
    
    | compact space | $\mathcal X$ |
    | --- | --- |
    | label space | $\mathcal Y = \lbrace 1, ..., C\rbrace $<br>($C$ classes classification problem) |
    | loss function | $l(\cdot,\cdot ; \mathbf w):\mathcal X \times \mathcal Y \rightarrow \mathcal R$ |
    | query budget | $b$ (oracleì—ê²Œ labelì„ ìš”ì²­í•  dataì˜ ê°œìˆ˜) |
    | multiple rounds | $k$ (queryë¥¼ ìœ„í•´ ì„ íƒëœ subset: $\mathbf s^k$) |

    > classical ë°©ë²•ì—ì„œëŠ” $b=1$ ì´ì§€ë§Œ, single pointëŠ” deep learningì—ì„œ ì˜ë¯¸ìˆëŠ” ì˜í–¥ì´ ì—†ê¸° ë•Œë¬¸ì— batch settingì„ ì‚¬ìš©í•œë‹¤.
    {: .prompt-info}
  
- **labelì´ ë˜ì–´ì•¼ í•  data pointsì˜ initial pool**ì€ **uniformly random í•˜ê²Œ ì„ ì •**í•œë‹¤.
    
    | $n$ | ì „ì²´ sample |
    | --- | --- |
    | $m$ | label $y$ë¥¼ ì•Œê³ ìˆëŠ” sample |
    | $\lbrace\mathbf x_i\rbrace_{i \in [n]}$, $\lbrace y_{s(j)}\rbrace_{j \in [m]}$  | active learning algorithmì´ access í•  ìˆ˜ ìˆëŠ” ì •ë³´<br>(initial sub-sampled poolì˜ labels ë§Œ ë³¼ ìˆ˜ ìˆë‹¤.) |
    | $\mathbf s^0=\lbrace s^0(j)\in [n]\rbrace_{j \in [m]}$ | initial labelled set |

- ë”°ë¼ì„œ ë¬¸ì œ ì •ì˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í•  ìˆ˜ ìˆë‹¤. (**future expected lossë¥¼ minimize**)
    
    ![first round](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-1.png){: style="max-width: 70%"}
    _first round_
    
    ![multiple round](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-2.png){: style="max-width: 70%"}
    _multiple round_
    
- active learning ì•Œê³ ë¦¬ì¦˜ì˜ **ê° iterationì—ì„œ ìˆ˜í–‰ë˜ëŠ” ë‘ ë‹¨ê³„**ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
    1. labelì„ query í•  data pointsì˜ setì„ ê³¨ë¼ oracleì—ê²Œ **query** í•œë‹¤.
    2. ìƒˆë¡­ê²Œ labelì„ ì–»ì€ ê²ƒê³¼ ì´ì „ì— labelì„ ì–»ì€ data pointsì— ëŒ€í•´ì„œ **classifierë¥¼ í•™ìŠµ**í•œë‹¤.
        - **fully-supervised ë°©ë²•** - label ëœ dataë§Œ ê°€ì§€ê³  í•™ìŠµí•œë‹¤.
        - **weakly-supervised ë°©ë²•** - label ëœ dataì™€ ì•„ì§ labelì´ ë˜ì§€ ì•Šì€ dataë¥¼ ëª¨ë‘ í•™ìŠµì— ì‚¬ìš©í•œë‹¤.

<br>

---

## 4. Method

### ë¬¸ì œ ì¬ì •ì˜

- minimize í•˜ë ¤ëŠ” **loss**ëŠ” ë‹¤ìŒì˜ í•­ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤.
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-3.png)
    
    - **generalization error** - ì´ë¯¸ CNNì—ì„œëŠ” ë¬´ì‹œ ê°€ëŠ¥í•˜ë‹¤. (bounded)
    - **training error** - CNNì—ì„œëŠ” ë§¤ìš° ë‚®ë‹¤.
    - **core-set loss** - active learningì—ì„œ critical í•˜ë‹¤.
- ë”°ë¼ì„œ core-set lossë¥¼ ì´ìš©í•œ ë‹¤ìŒì˜ ì‹ìœ¼ë¡œ **active learning ë¬¸ì œë¥¼ ì¬ì •ì˜**í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ëª¨ë“  labelì— accessë¥¼ í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ì ìœ¼ë¡œ ì´ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ë‹¤. **â†’ upper boundë¥¼ ì´ìš©í•˜ì!**
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-4.png)
    

<br>

### Core-Set LossëŠ” Bounded ëœ ê°’ì´ë‹¤.

- minimize í•˜ë ¤ëŠ” **loss functionì´ Lipschitz continuous** í•˜ë‹¤ë©´, **upper boundê°€ ì¡´ì¬**í•œë‹¤ëŠ” ê²ƒì„ ì¦ëª…í–ˆë‹¤.
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-5.png)
    
- training errorëŠ” 0ìœ¼ë¡œ ê°„ì£¼í•˜ë¯€ë¡œ, **core-set loss**ëŠ” **ì „ì²´ datasetì— ëŒ€í•œ average error**ì™€ ê°™ë‹¤.
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-15.png)
    
- **upper bound**ëŠ” **covering radius $\delta$**ë¡œ ì •ì˜ë  ìˆ˜ ìˆë‹¤.
    - **set $\mathbf s$ì˜ ê° ì›ì†Œê°€ ì¤‘ì‹¬**ì´ê³  **radiusê°€ $\delta$**ì¸ ballsë¡œ **entire set $s^*$ì„ cover** í•  ìˆ˜ ìˆë‹¤.
    - labelled pointsì˜ ê°œìˆ˜ì˜ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤.
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-6.png)
    
- ì´ **upper boundë¥¼ minimize** í•˜ì. ($\delta_s$ì™€ ê´€ë ¨)
    
    $$
    \min_{\mathbf s^1: |\mathbf s^1 \le b|} \delta_{\mathbf s^0 \cup \mathbf s^1}
    $$
    

<br>

### [Algorithm 1] K-Center-Greedy

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-7.png){: style="max-width: 70%"}

- loss functionì˜ upper boundë¥¼ minimize í•˜ëŠ” ê²ƒ == k-Center problem
- **í•˜ë‚˜ì˜ data pointì™€ ê°€ì¥ ê°€ê¹Œìš´ center ì‚¬ì´ì˜ ìµœëŒ€ ê±°ë¦¬(== radius $\delta$)ë¥¼ ìµœì†Œí™”**í•˜ëŠ” $b$ ê°œì˜ center pointsë¥¼ ê³ ë¥´ëŠ” ë¬¸ì œì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë“  data pointë¥¼ cover í•´ì•¼ í•œë‹¤.
- NP-Hard ë¬¸ì œì´ì§€ë§Œ **greedy approachë¥¼ ì´ìš©í•œ 2-OPT solution**ì´ ì¡´ì¬í•œë‹¤.
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-8.png){: style="max-width: 60%"}
    
- ì´ë ‡ê²Œ êµ¬í•œ solutionì€ **ì‹¤ì œ optimal minimumì˜ 2ë°°ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤**ëŠ” ê²ƒì´ ì¦ëª…ë˜ì–´ ìˆë‹¤.
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-16.png){: style="max-width: 60%"}
    

<br>

### [Algorithm 2] Robust K-Center

- [Algorithm 1] k-Center-Greedyë¥¼ initializationìœ¼ë¡œ ì‚¬ìš©í•œ í›„, MIPì˜ feasibilityë¥¼ ë°˜ë³µì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ë°©ë²•ì´ë‹¤.
    - $OPT \le \delta$ â†’ **mixed integer program(MIP)**ìœ¼ë¡œ ì •ì˜í•œë‹¤.
    - **outliersì˜ ê°œìˆ˜ì˜ upper limit** $\Xi$(= cover í•˜ì§€ ì•Šì„ sample ìˆ˜)ì„ ë„ì…í•˜ì—¬ **robustness**ë¥¼ ê°œì„ í•œë‹¤.
- $\delta$ê°€ ì–¼ë§ˆê°€ ë˜ì—ˆì„ ë•Œ ì œì•½ì‹ì„ ëª¨ë‘ ë§Œì¡±ì‹œí‚¤ëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, **lower / upper boundë¥¼ ì„¤ì • í›„ ì—…ë°ì´íŠ¸**ë¥¼ ê³„ì† í•˜ë©´ì„œ **ê·¸ ë‘ ê°’ì´ ê°™ì•„ì§ˆ ë•Œ**ì˜ setì´ optimal solutionì´ë¼ê³  íŒë‹¨í•œë‹¤.
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-9.png){: style="max-width: 80%"}
    
    - $\delta$ ê°’ì„ ê³„ì† ë°”ê¿”ê°€ë©´ì„œ MIPê°€ **ì–¸ì œ feasible($\min_{\mathbf s^1}\max_{i}\min_{j\in \mathbf s^1 \cup \mathbf s^0}\Delta(\mathbf x_i, \mathbf x_j)\le\delta$) í•œì§€ í™•ì¸**í•œë‹¤.
    - ë‹¤ìŒì˜ ê°’ì€ ëª¨ë‘ 0 ë˜ëŠ” 1ì´ë‹¤.
        
        | --- | --- |
        | $u_i$ | i-th data pointerê°€ centerë¡œ ì„ íƒë˜ë©´ 1 |
        | $\mathcal w_{i,j}$ | i-th pointê°€ j-th pointë¡œ covered ë˜ë©´ 1 |
        | $\xi_{i,j}$ | i-th pointê°€ outlierì´ë©´ì„œ $\delta$ constraint ì—†ì´ j-th pointì— ì˜í•´ covered ë˜ë©´ 1 |

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-10.png)

<br>

### Implementation Details

| --- | --- |
| **distance** | final fully-connected layerì—ì„œ ë‚˜ì˜¨ activationsì˜ L2 distance |
| **network** | (CNN) VGG-16<br>(weakly-supervised learning) Ladder networks |
| **upper bound on outliers** $\Xi$ | 1e-4 * n<br>(n: unlabelled pointsì˜ ê°œìˆ˜) |
| **etc** | He initialization, RMSProp(lr=1e-3), Gurobi (MIP framework) |

<br>

---

## 5. Experimental Results

### Task & Dataset

- image & digit classification task
- CIFAR-10, CIFAR-100, SVHN

<br>

### Results

![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-11.png)

- ëª¨ë“  ì‹¤í—˜ì—ì„œ our algorithmì´ ëª¨ë“  baselineì— ë¹„í•´ ì„±ëŠ¥ì´ ë›°ì–´ë‚¬ë‹¤.
- íŠ¹íˆ, **weakly-supervised model**ì—ì„œ ì„±ëŠ¥ì´ í¬ê²Œ ë›°ì–´ë‚¬ë‹¤. (geometric method â†’ better feature learning)
- **CIFAR-100 ë°ì´í„°ì…‹**ì—ì„œ ìƒìŠ¹ í­ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì•˜ë‹¤. (core-set lossì˜ boundê°€ class ê°œìˆ˜ì— ì˜í–¥ì„ ë°›ìœ¼ë¯€ë¡œ)
- ê¸°ì¡´ì˜ batch mode active learning baseline ë°©ë²•ë“¤ì´ ë¬´ì¡°ê±´ greedy baseline ë°©ë²•ë“¤ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒì€ ì•„ë‹ˆì—ˆë‹¤.
    - ì—¬ì „íˆ uncertainty informationì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
    - soft-max probabilityê°€ uncertaintyì˜ ì¢‹ì€ proxyê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì´ë‹¤.
- **batch setting**ì—ì„œëŠ” uncertaintyë¥¼ ì´ìš©í•œ ë°©ë²•ë³´ë‹¤ ì˜¤íˆë ¤ random samplingì´ ì„±ëŠ¥ì´ ì¢‹ì€ ê²½ìš°ê°€ ë§ë‹¤.
    - uncertaintyë¥¼ ì´ìš©í•œ ë°©ë²•ì€ spaceì˜ í° ë¹„ìœ¨ì„ cover í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ì— ë°˜í•´ our algorithmì€ **ì „ì²´ spaceì—ì„œ ê³¨ê³ ë£¨ query** í•œë‹¤.
    
    ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-12.png){: style="max-width: 60%"}
    

<br>

### Optimality

- our algorithm = **[initialization] greedy 2-OPT solution** + **[iteration] check feasibility of MIP**
- **MIPì˜ average run-time**ì„ ì¸¡ì •í•œ ê²°ê³¼, dataset of 50k imagesì— ëŒ€í•´ì„œëŠ” í•©ë¦¬ì ì¸ ì‹œê°„ ì•ˆì— convergeê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.
    - ë”°ë¼ì„œ our algorithmë„ ì‹¤ì œë¡œ ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ run-timeì´ ì†Œìš”ëœë‹¤.
        
        ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-13.png){: style="max-width: 60%"}
        
- **k-Center-Greedy**ì™€ **our method**ì˜ **accuracy**ë¥¼ ë¹„êµí•œ ê²°ê³¼, ì•½ê°„ì˜ ì„±ëŠ¥ í–¥ìƒì´ ìˆì—ˆë‹¤.
    - datasetì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì„œ MIPê°€ feasible í•˜ì§€ ì•Šì•„ 2-OPT solutionë§Œì„ ì´ìš©í•˜ë”ë¼ë„, ë‹¤ë¥¸ baselineì— ë¹„í•´ ì„±ëŠ¥ì´ ì¢‹ë‹¤.
        
        ![Untitled](/assets/img/posts/Deep-Learning/Paper-Review/2022-10-06-14.png){: style="max-width: 70%"}
        

<br>

---

## 6. Conclusion

- active learningì˜ **classical uncertainty based methods**ëŠ” **batch sampling**ìœ¼ë¡œ ì¸í•œ **correlation** ë•Œë¬¸ì— CNNì— ì ìš©ì´ ì œí•œì ì´ì—ˆë‹¤.
- active learning ë¬¸ì œë¥¼ **core-set selection** ë¬¸ì œë¡œ ì¬ì •ì˜í•œ ê²°ê³¼, í° ì„±ëŠ¥ í–¥ìƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.
