---
title: "[Paper] Face, Body, Voice: Video Person-Clustering with Multiple Modalities"
date: 2022-04-26 19:50:00 +0900
categories: [Deep Learning, Paper Review]
tags: [paper review, clustering, face clustering, person clustering]
math: true
---

> ğŸ“ Paper: <https://arxiv.org/abs/2105.09939>

<br>

## 0. Abstract

- person-clustering in videos (grouping characters according to their identity)
- previous methods
    - narrower task of face-clustering
    - ignore other cues such as voice, appearance, and the editing structure of the videos
- contributions
    - Multi-Modal High-Precision Clustering algorithm for person-clustering in videos using cues from several modalities (face, body, voice)
    - Video Person-Clustering dataset
        - body-tracks, face-tracks when visible, voice-tracks when speaking
- effectiveness of using multiple modalities for person-clustering

<br>

---

## 1. Introduction

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-01.png){: .w-75}

- ê¸°ì¡´ì˜ identity clustering ë°©ë²•ì˜ ëŒ€ë¶€ë¶„ì€ **face-level ì •ë³´ë¥¼ ì´ìš©**í•˜ëŠ” ê²ƒìœ¼ë¡œ **í•œê³„**ê°€ ìˆë‹¤.
    1. ë§ì€ **informative cues**(voice, appearance, editing structure)**ë¥¼ ë¬´ì‹œ**í•œë‹¤.
        
        > **editing structure** - neighboring shotsë¥¼ ìƒê°í•˜ë©´ ëœë‹¤.
        > 
    2. story understandingê³¼ ê°™ì€ **downstream applicationì—ì„œì˜ ì‚¬ìš©ì„±ì„ ì œí•œ**í•œë‹¤.
        
        > ë³´ì´ì§€ ì•ŠëŠ” ëª¨ë“  ë“±ì¥ì¸ë¬¼ì— ëŒ€í•´ ì•Œì•„ì•¼ story-lineì„ ì´í•´í•  ìˆ˜ ìˆë‹¤.
        > 
    
    **â‡’ ë”°ë¼ì„œ multi-modal í˜•íƒœì˜ person-level ì •ë³´ë¥¼ ì‚¬ìš©í•˜ëŠ” person-clustering ë°©ë²•ì„ ì œì•ˆí•œë‹¤.**
    

- í•œ ì‚¬ëŒì— ëŒ€í•œ **multiple modalitiesê°€ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ê³³**ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
    1. how to obtain **pure** clusters
    2. how to **merge** clusters w/o violating their purity
        
        > multiple modalitiesëŠ” <span class="blue">**unmergeable clusters**</span>ì— ëŒ€í•´ <span class="blue">**bridge**</span> ì—­í• ì„ í•œë‹¤.
        > 

- <span class="shl">**Multi-Modal High-Precision Clustering (MuHPC)**</span>
    - **multiple modalities**ë¥¼ ì‚¬ìš©í•œë‹¤. (face, voice, body appearance)
    - **first nearest neighbour clustering algorithms**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„í–ˆë‹¤.
    - **cannot-link constraints**ì™€ **video editing structure**ë„ ì´ìš©í•œë‹¤.

- <span class="shl">**Video Person-Clustering Dataset (VPCD)**</span>
    - ê¸°ì¡´ face-clustering datasetì˜ ê²½ìš°, skin colorê°€ ë‹¤ì–‘í•˜ì§€ ì•Šë‹¤.
    - ê¸°ì¡´ face dataset ëŒ€ë¹„ **ê°œì„ í•œ ì‚¬í•­**ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
        - person-level multi-modal annotationsë¥¼ ì¶”ê°€í–ˆë‹¤. (person-tracks, voice utterances)
        - ë‹¤ë¥¸ TV showì™€ filmsë¥¼ ì¶”ê°€í–ˆë‹¤. (â†’ program setsë¼ê³  ëª…ëª…í•œë‹¤.)
    - ëª¨ë“  annotated charactersì— ëŒ€í•´ **í¬í•¨í•˜ëŠ” ì •ë³´**ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
        - body-tracks
        - face-tracks (when visible)
        - voice utterances (when speaking)


- ìƒˆë¡œìš´ architectureë¥¼ ì œì•ˆí•˜ê±°ë‚˜ ìƒˆë¡œìš´ networkë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤.
    - **face / speaker recognition**ì€ pre-trained networksì—ì„œ ì–»ì€ featureë¥¼ ì‚¬ìš©í•œë‹¤.
    - **body Re-ID**ì— ëŒ€í•´ì„œë§Œ networkë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.

<br>

---

## 2. Comparison to Related Work

- **Face-Clustering**
    - [previous] **â€œfaceâ€ clustering**ì— êµ­í•œë˜ì–´ ìˆë‹¤.
    - [our work] <span class="blue">**multi-modal person-clustering**</span>ì„ í†µí•´ faceê°€ visible í•œì§€ì™€ëŠ” ìƒê´€ ì—†ì´ clusteringì´ ê°€ëŠ¥í•˜ë‹¤.
- **Face-Labelling**
    - [previous] **label ì •ë³´ë‚˜ external sources**ê°€ í•„ìš”í–ˆë‹¤.
    - [our work] labelì´ ì•„ë‹Œ cluster ì‘ì—…ì´ë¯€ë¡œ, character-classifiers, ID supervision, í˜¹ì€ <span class="blue">**extra annotationì´ í•„ìš”í•˜ì§€ ì•Šë‹¤**</span>. ê·¸ ëŒ€ì‹  <span class="blue">**editing structure**</span>ë‚˜ <span class="blue">**multi-modality**</span> ë“±ì˜ ì´ìš© ê°€ëŠ¥í•œ ëª¨ë“  cueë¥¼ ì‚¬ìš©í•œë‹¤.
- **Person Re-ID**
    - [our work] videoì—ì„œ track-levelì—ì„œì˜ ëª¨ë“  charaterë¥¼ clusterí•˜ë¯€ë¡œ <span class="blue">**search queriesê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤.**</span>
- **Related Datsets**
    - [preivous] **demographic groups**ë¥¼ under-representí•˜ë©°, **face annotation** ë§Œì„ í¬í•¨í•œë‹¤.
    - [our work] 6ê°œì˜ ë‹¤ë¥¸ TV showì™€ movieì— ëŒ€í•´ <span class="blue">**ë‹¤ì–‘í•œ characters**ì˜ **multi-modal**</span> annotationsë¥¼ í¬í•¨í•œë‹¤.
- **Story Understanding**
    - [our work] scene ì•ˆì— ëˆ„ê°€ ì¡´ì¬í•˜ëŠ”ì§€ì— ëŒ€í•´ ì§‘ì¤‘í•œë‹¤.

<br>

---

## 3. Multi-Modal High-Precision Clustering (MuHPC)

### 3-1. Features

- single hierarchical agglomerative clustering (HAC) approachì´ë‹¤.
    - <span class="shlb">similarities of modality features</span>ì™€ <span class="shlb">constraints from the video structure</span>ë¥¼ ì´ìš©í•˜ì—¬ person-tracksë¥¼ ë¬¶ëŠ”ë‹¤.
- <span class="shlp">pre-computed featuresë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, optimal hyper-parameterë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ ì™¸ì—ëŠ” trainingì´ í•„ìš”í•˜ì§€ ì•Šë‹¤.</span>
- ì—¬ê¸°ì—ì„œëŠ” face, voice, body appearance ì´ 3ê°œì˜ modalitiesë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ì‰½ê²Œ ë” ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.

<br>

### 3-2. Notation

- datasetì€ **person-tracks** $$x_i$$ì™€ **$$C$$ê°œì˜ characters**ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.
    
    > ëª¨ë“  $$x_i$$ë¥¼ identityë¡œ ë¬¶ì–´ $$C$$ê°œì˜ clusterë¡œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œ! ($$C$$ëŠ” unknown)
    > 
- ê° **person-track $$x_i$$**ëŠ” multi-modalityë¥¼ í¬í•¨í•˜ëŠ” í•˜ë‚˜ì˜ feature vectorë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
    
    $$
    x=\{x_f, x_v, x_b\}
    $$
    
    - ê°ê° face, voice, bodyì— ê´€í•œ featureì´ë‹¤.
    - face/bodyê°€ visible í•œì§€, í˜¹ì€ speaking ì¸ì§€ì— ë”°ë¼ ì¢…ì†ì ì´ë‹¤.
- í•˜ë‚˜ì˜ modalityì— ëŒ€í•´ ë‘ track features ê°„ì˜ **distance**ëŠ” $$d(x_i, x_j)$$ë¼ê³  í•œë‹¤.
- NNì€ nearest neighborë¡œ, $$n_{x_i}^1$$ì€ **track $$x_i$$ì˜ ì²« ë²ˆì§¸ NN track**ì´ë‹¤.
- $$x_i$$ê°€ ì¡´ì¬í•˜ëŠ” **video frameì˜ ì§‘í•©**ì„ $$T_i$$ë¼ê³  í•œë‹¤.
- ê° **cluster partition**ì€ $$\Gamma$$ë¼ê³  í•œë‹¤.

<br>

### 3-3. Three stages

1. <span class="shl">**single modality(= face)**ë¥¼ ì´ìš©í•´ **high-precision clusters**ë¥¼ $$K_1$$ê°œ ìƒì„±í•œë‹¤.</span>
    - HACì„ ì—¬ëŸ¬ ë²ˆ iterate í•˜ì—¬ **first nearest neighbour(NN)**ë¥¼ ê³µìœ í•˜ëŠ” person-tracks ë¼ë¦¬ ë¬¶ëŠ”ë‹¤.
    - ë‹¤ìŒì˜ **ë‘ additional constaints**ì— ì¢…ì†ì ì´ë‹¤. (= ë‹¤ìŒì„ ë§Œì¡±í•´ì•¼ NNì´ valid í•˜ë‹¤.)
        - <span class="blue">**spatio-temporal cannot-link constraint**</span> for concurrent tracks
            
            > ì¼ì‹œì ìœ¼ë¡œ ê²¹ì¹˜ëŠ” trackì€ ê°™ì€ groupìœ¼ë¡œ ë¬¶ì´ë©´ ì•ˆëœë‹¤. í•œ ì‚¬ëŒì€ í•˜ë‚˜ì˜ frameì—ì„œ ë‹¨ í•œ ë²ˆ ë“±ì¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
            > 
        - <span class="blue">**NN distance constraint**</span> (= conservative threshold in the maximum NN distances)
            
            > <span class="shlp">$$d_f<\tau_f^{tight}$$</span>
            > 
            > 
            > â‡’ trackê³¼ ì²«ë²ˆì§¸ NN ê°„ì˜ distance $$d_f(x_i, n_{x_i}^1)$$ëŠ” **$$\tau_f^{tight}$$ ë³´ë‹¤ ì‘ì•„ì•¼ í•œë‹¤.**
            > 
    - ê° <span class="pink">**cluster parition** $$\Gamma$$</span>ì—ì„œ, ë‹¤ìŒì˜ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¥¼ ë§Œì¡±í•˜ëŠ” tracksë¥¼ mergeí•˜ì—¬ <span class="pink">**$$K_\Gamma$$ ê°œì˜ clusters**</span>ë¥¼ ìƒì„±í•œë‹¤. ì´ëŠ” **adjancency matrix**ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.
        1. ì„œë¡œ, í˜¹ì€ ì¼ë°©ì ìœ¼ë¡œ first NNì¸ ê²½ìš°
        2. ê³µí†µì˜ NN $$n_{x_i}^1$$ì„ ê°€ì§€ëŠ” ê²½ìš°
            
            ![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-02.png){: .w-75}
            
    - **stopping criteria**ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. (ë‘˜ ì¤‘ í•˜ë‚˜ ë§Œì¡± ì‹œ stop)
        1. ëª¨ë“  clusters ê°„ì˜ distanceê°€ $$\tau_f^{tight}$$ ë³´ë‹¤ í° ê²½ìš°
        2. ëª¨ë“  clustersê°€ cannot-link constraintë¡œ ë¶„ë¦¬ëœ ê²½ìš°
    - <span class="pink">**$$K_1$$ ê°œì˜ high-precision clusters**</span>($$K_1 \ge C$$)ê°€ ìƒì„±ëœë‹¤.

    <br>

2. <span class="shl">**multi-modality(= face and voice)**ë¥¼ ì´ìš©í•´ single face modalityë¡œëŠ” unmergeable í–ˆë˜ clusterë“¤ì„ í•©ì³ì¤€ë‹¤.</span>
    - **bridge** ì—­í• ì„ í•˜ëŠ” ê²ƒì´ë‹¤.
    - faceì™€ voice distanceì— ëŒ€í•œ ìƒˆë¡œìš´ thresholdë¥¼ ì§€ì •í•œë‹¤.
        
        > <span class="shlp">$$d_f<\tau_f^{loose}$$</span> ì´ê³  <span class="shlp">$$d_v<\tau_v^{loose}$$</span> ì¼ ë•Œ merge clusters!
        > 
    - <span class="pink">**$$K_2$$ ê°œì˜ high-precision clusters**</span>($$K_2 \le K_1$$)ê°€ ìƒì„±ëœë‹¤.
        
        ![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-03.png){: .w-75}
    
    <br>

3. <span class="shl">**visible facesê°€ ì—†ì–´ ì• ë‹¨ê³„ì—ì„œ clustered ë˜ì§€ ëª»í•œ tracks**ë¥¼ ê¸°ì¡´ì˜ clustersì™€ ì—°ê²°í•´ì¤€ë‹¤.</span>
    - **body-appearance**ë¥¼ ì´ìš©í•˜ì—¬ **face-less person-tracks**ë¥¼ ì• ë‹¨ê³„ì˜ **high-precision clusters**ì— ì´ì–´ì¤€ë‹¤.
    - ë‹¤ìŒì˜ **ë‘ constraints**ë¥¼ ì‚¬ìš©í•œë‹¤.
        - <span class="blue">**editing structure**</span> (neighboring shotsì¸ì§€)
        - <span class="blue">**conservative threshold**</span> on **body features** (same person w/ same clothing)
    - **ratio-test**ë¥¼ ìˆ˜í–‰í•œë‹¤.
        1. ê° body-trackì— ëŒ€í•´ first & second NN distanceë¥¼ êµ¬í•œë‹¤.
        2. <span class="shlp">$$d_{b,x_i}^2/d_{b, x_i}^1 > \rho$$</span> ì¸ body-trackì€ ignore í•œë‹¤. ($$\rho$$ = threshold)
            
            > **back(ë’·ëª¨ìŠµ)**ì˜ ê²½ìš°, cluster ë˜ê¸° ì–´ë ¤ìš´ ê²½ìš°ê°€ ë§ë‹¤. 
            ë”°ë¼ì„œ ìƒˆë¡œìš´ thresholdë¥¼ ì„¤ì •í•´ <span class="shlp">$$d_b>\tau_b^{back}$$</span>ì¸ ê²ƒë“¤ì€ ignore í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤.
            > 
    - **Stage 3ì—ì„œëŠ” <span class="pink">clusterì˜ ê°œìˆ˜ê°€ $$K_2$$</span>ì—ì„œ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤!**

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-04.png){: .w-75}
_Buffy_

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-05.png){: .w-75}
_Sherlock_

<br>

### 3-4. Number of Clusters

- <span class="pink">**ìµœì¢… cluster ê°œìˆ˜ì¸ $$K_2$$**</span>ë¥¼ $$C$$ì— ê°€ê¹ê²Œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.
- **[Idea]** largest cluster ê°„ì—ëŠ” identity overlapì´ ì˜ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤. í•˜ì§€ë§Œ small clusterì™€ large cluster ê°„ì—ëŠ” overlapì´ ì¡´ì¬í•˜ê¸° ì‰½ë‹¤.
    - large clusterëŠ” í•´ë‹¹ identityì— ëŒ€í•´ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤.
    - ë§Œì•½ ë‘ large clustersê°€ ê°™ì€ identityë¥¼ í¬í•¨í•œë‹¤ë©´, merge ë˜ì—ˆì„ ê²ƒì´ë‹¤.
    - ì¦‰, ë°˜ë³µì ìœ¼ë¡œ smallestì™€ largest clusterë¥¼ merge í•˜ì—¬ $$K_2$$ê°€ $$C$$ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•˜ì!

<br>

### 3-5. Pre-trained

- [previous] video datasetì— ëŒ€í•´ character featuresë¥¼ **fine-tune** í•œë‹¤.
- [our work] **pre-trained features**ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¤ìŒì˜ ì¥ì ì´ ìˆë‹¤.
    1. **reducing the <span class="blue">computational burden</span>**
    2. **leading to incread <span class="blue">generalisation</span> capabilities**

<br>

### 3-6. Hyper-parameters

> VPCDì˜ validation paritionì„ í†µí•´ í•™ìŠµí•œë‹¤. ì—¬ê¸°ì—ì„œëŠ” constant ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
> 

- <span class="shl">**$$\tau_f^{loose}$$ (face modalityì™€ ê´€ë ¨)**</span>

    - face featureê°€ ëŒ€ìš©ëŸ‰ì˜ datasetì—ì„œ ì´ë¯¸ í•™ìŠµë˜ì–´ ìˆê¸° ë•Œë¬¸ì— discriminative í•˜ê³  universal í•˜ë‹¤.
    
        > ì¼ë°˜í™” good!

- <span class="shl">**$$\tau_v^{loose}$$ (voice modalityì™€ ê´€ë ¨)**</span>
    - less universal í•˜ë¯€ë¡œ, í•˜ë‚˜ì˜ ê°’ì´ ë‹¤ë¥¸ program setsì— ëŒ€í•´ ì˜ generalise ë˜ì§€ ì•ŠëŠ”ë‹¤.

        > ë”°ë¼ì„œ ê° program setì— ëŒ€í•´ automatically í•˜ê²Œ í•™ìŠµí•œë‹¤.
        
    - $$\tau_v^{loose}$$ ê°€ ë‹¤ë¥¸ ì‚¬ëŒì˜ **voices ê°„ì˜ minimum distanceë³´ë‹¤ ì‘ì€ ê°’**ì„ ê°€ì§€ë„ë¡ í•œë‹¤.

        > <span class="blue">**cannot-link speaking face-tracks**</span>ì™€ <span class="blue">**Stage1ì˜ clusters**</span>ë¥¼ ì‚¬ìš©í•˜ì—¬ exampleì„ ë” ë§Œë“¤ì–´ì„œ distancesë¥¼ êµ¬í•˜ê³ , ì´ ë¶„í¬ë¥¼ ì´ìš©í•˜ì—¬ $$\tau_v^{loose}$$ë¥¼ êµ¬í•œë‹¤. 2ê°œì˜ face-tracksê°€ same shotì—ì„œ ë§í•˜ëŠ” ê²½ìš°ëŠ” ê±°ì˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.
        >
        > ë¹„ìŠ·í•œ ëª©ì†Œë¦¬ë¥¼ ê°€ì§„ characterê°€ ë§ì€ program setì˜ ê²½ìš°, $$\tau_v^{loose}$$ëŠ” ë‚®ì€ ê°’ì„ ê°€ì§ˆ ê²ƒì´ë‹¤.

<br>

---

## 4. Video Person-Clustering Dataset (VPCD)

### 4-1. Contents

- ë‹¤ì–‘í•œ TV showì™€ movie ì†ì˜ ë‹¤ì–‘í•œ ë“±ì¥ì¸ë¬¼ë“¤ì— ëŒ€í•´ì„œ full multi-modal annotationsì„ ë‹´ê³  ìˆë‹¤.
- val. setê³¼ test setì„ í¬í•¨í•œë‹¤.
- **voice-trackì˜ íŠ¹ì§•**
    - **laughter**ë„ í¬í•¨í•œë‹¤. (ex. TBBT/Friendsì˜ live studio audience, characterì˜ laughter)
    - ì—¬ëŸ¬ ê°œì˜ voice-trackì´ ê²¹ì¹˜ëŠ” ê²½ìš°ë‚˜, ë„ˆë¬´ ì§§ì€ ê²½ìš°ëŠ” ignore í•œë‹¤.

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-06.png){: .w-75}

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-07.png)

<br>

### 4-2. Annotation Process

> automatic & manual annotation methodsë¥¼ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.

- <span class="shlp">**Face**</span>
    - previous worksì—ì„œ ì œê³µí•˜ê³  ìˆëŠ” original datasetsì˜ face bbox, track annotations, ID labelsë¥¼ ì´ìš©í•˜ë¯€ë¡œ <span class="blue">automatic</span> ì´ë‹¤.

- <span class="shlp">**Body**</span>
    - **annotation ë°©ë²•**
        1. MovieNet ë°ì´í„°ì…‹ìœ¼ë¡œ train ëœ Cascade R-CNNì„ ì´ìš©í•˜ì—¬ bodyë¥¼ detect í•œë‹¤.
        2. IoU trackerë¥¼ ì‚¬ìš©í•˜ì—¬ tracksë¥¼ ìƒì„±í•œë‹¤.
    - **ë‘ ê°€ì§€ ê²½ìš°**
        - body-trackì´ face-trackê³¼ ëª…í™•í•˜ê²Œ ëŒ€ì‘ë˜ëŠ” ê²½ìš° (ex. ë‹¤ë¥¸ face-trackê³¼ ìœ ì˜ë¯¸í•œ IoU ê°’ì„ ê°€ì§€ì§€ ì•ŠëŠ” ê²½ìš°)
            - í•´ë‹¹ face-trackì˜ ì •ë³´ë¡œ <span class="blue">automatically</span> annotated ëœë‹¤.
            - Hungarian Algorithmì„ ì‚¬ìš©í•œë‹¤.
        - ê·¸ ì™¸ì˜ ê²½ìš° (ex. face-less)
            - <span class="blue">manually</span> annotate í•œë‹¤.
            - í‰ê· ì ìœ¼ë¡œ ì „ì²´ body-trackì˜ 10~15% ì •ë„ì— í•´ë‹¹í–ˆë‹¤.
            
- <span class="shlp">**Voice**</span>
    - ëª¨ë“  characterì— ëŒ€í•´ speaking partsë¥¼ <span class="blue">manually</span> segment í•œë‹¤.

<br>

### 4-3. Feature Extraction

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-08.png){: .w-50}

<br>

---

## 5. Experiments

### 5-1. Detail Settings

- feature distance $$d_f, d_b, d_v$$ëŠ” ëª¨ë‘ `1 - cosine similarity`ë¡œ ê³„ì‚°í•œë‹¤.
- VPCD val. setì—ì„œ í•™ìŠµí•œ hyperparametersëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
    
    $$
    \tau_f^{tight}=0.48, \text{  }\delta=0.025, \text{  }\rho=0.9, \text{  }\tau_b^{back}=0.4
    $$
    
    â‡’ featuresê°€ ë°”ë€” ë•Œì—ë§Œ ì¬í•™ìŠµí•˜ë©´ ëœë‹¤.
    

> <span class="shl">**[Appendix] í•™ìŠµëœ parameterëŠ” different program setsì—ì„œ ì˜ generalise í•œë‹¤.**</span>
> 
> - face featuresê°€ universal í•˜ë©°, VPCD ë‚´ì˜ program setsê°€ visually disparate í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
> - MuHPCëŠ” VPCDì— ì—†ëŠ” ë‹¤ì–‘í•œ program setsì—ì„œë„ ì˜ ë™ì‘í•  ê²ƒì´ë‹¤!
{: .prompt-info}

<br>

### 5-2. Metrics

- <span class="shlp">**Weighted Cluster Purity (WCP)**</span>
    - í•´ë‹¹ cluster ë‚´ë¶€ì˜ track ê°œìˆ˜ë¥¼ í†µí•´ <span class="blue">**í•´ë‹¹ clusterì˜ purity**</span>ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    - í•´ë‹¹ clusterì˜ WCP = 1ì¼ ë•Œ, ëª¨ë“  sampleì´ ê°™ì€ classì—ì„œ ì™”ë‹¤ëŠ” ëœ»ì´ë‹¤.

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-09.png){: .w-25}

- <span class="shlp">**Normalized Mutual Information (NMI)**</span>
    - clustering qualityì™€ cluster ê°œìˆ˜ ê°„ì˜ <span class="blue">**trade-off**</span>ë¥¼ ì¸¡ì •í•œë‹¤.

- <span class="shlp">**Character Precision & Recall (CP, CR)**</span>
    - WCP, NMIì™€ ë‹¤ë¥´ê²Œ ground truth identitiesì˜ ê°œìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.
    - **CP** - ê° characterì— assign ëœ cluster ë‚´ì—ì„œì˜ tracksì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.
    - **CR** - í•´ë‹¹ characterì˜ total tracks ì¤‘ clusterì— ë‚˜íƒ€ë‚˜ëŠ” trackì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.
    - ëª¨ë“  characterì— ëŒ€í•´ í‰ê· ìœ¼ë¡œ êµ¬í•œë‹¤.

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-10.png){: .w-50}
_Stage 1ì—ì„œì˜ ê²°ê³¼ â†’ Stage 1ì—ì„œëŠ” faceë¥¼ ì´ìš©í•œë‹¤._

<br>

### 5-3. Test Protocol

- **Automatic Termination (AT)** - cluster ê°œìˆ˜ê°€ ì•Œë ¤ì§€ì§€ ì•Šì€ realistic í•œ ìƒí™©
- **Oracle Cluster (OC)** - cluster ê°œìˆ˜ê°€ ì•Œë ¤ì§„ ìƒí™© (SOTAì™€ì˜ ë¹„êµë¥¼ ìœ„í•´ì„œ)

<br>

### 5-4. Person-Clustering

> realistic í•œ ìƒí™©ì—ì„œì˜ ë¹„êµë¥¼ ìœ„í•´ AT protocol í•˜ì—ì„œ ì‹¤í—˜í•˜ì˜€ë‹¤.
> 

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-11.png)

- MuHPCê°€ ê¸°ì¡´ì˜ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ face-clustering algorithmsë³´ë‹¤ **significantly outperform** í•˜ì˜€ë‹¤.
- MuHPCì— voice, bodyë¥¼ ì¶”ê°€í•˜ëŠ” ê²½ìš°, ëŒ€ë¶€ë¶„ì˜ program setì—ì„œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤.
- í•˜ì§€ë§Œ **Hidden Figures**ì˜ ê²½ìš°, bodyë¥¼ ì¶”ê°€í•œë‹¤ê³  í•´ì„œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šì•˜ë‹¤.
    - ë§ì€ **dark scenes**ì™€ **non-distinctive clothing**ì´ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì´ë‹¤.
    - ì´ëŸ° ê²½ìš°ì—ëŠ” face-less bodiesë¥¼ cluster í•˜ê¸° ìœ„í•´ì„œ temporal-NN ë“±ì˜ ë°©ë²•ì„ ì‚¬ìš©í•´ ë³¼ ìˆ˜ ìˆë‹¤.

<br>

### 5-5. Face-Clustering

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-12.png){: .w-75}

- ì˜¤ì§ face-tracksë§Œ ì‚¬ìš©í•˜ì—¬ previous worksì™€ ë¹„êµí•˜ì˜€ë‹¤.
- ATì™€ OC protocol ëª¨ë‘ì—ì„œ MuHPCê°€ ê¸°ì¡´ì˜ face-clustering algorithmsë³´ë‹¤ **significantly outperform** í•˜ì˜€ë‹¤.
- datasetì˜ ë‚œì´ë„ê°€ ë” ì–´ë ¤ìš¸ìˆ˜ë¡, multi-modalityì˜ íš¨ê³¼ê°€ ë” í¬ê²Œ ë‚˜íƒ€ë‚¬ë‹¤.

<br>

### 5-6. Enabling Story Understanding

- story understandingì„ ìœ„í•´ì„œëŠ” ëˆ„ê°€ í•´ë‹¹ sceneì— ì¡´ì¬í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ì•¼ í•˜ëŠ”ë°, ì´ëŠ” person-clusteringìœ¼ë¡œ í•´ê²° ëœë‹¤.
- ì´ë¥¼ í†µí•´ **character co-occurrences**ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ê³§ **character ê°„ì˜ interaction** ì •ë„ë¼ê³  í•  ìˆ˜ ìˆë‹¤.

![](/assets/img/posts/Deep-Learning/Paper-Review/2022-04-26-13.png)
_TBBTì˜ character co-occurrences_

- **person-level GT**ë¥¼ ë³´ë©´, Sheldonê³¼ LeonardëŠ” ì „ì²´ frameì˜ 24%ì—ì„œ, ì—°ì¸ ê´€ê³„ì¸ Pennyì™€ LeonardëŠ” ì „ì²´ frameì˜ 9%ì—ì„œ í•¨ê»˜ ë“±ì¥í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
- **face-level GT**ì˜ ê²½ìš°, GT ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  Pennyì™€ Leonardì˜ ê´€ê³„ê°€ 3%ë¡œ, ë‹¤ë¥¸ ê´€ê³„ì™€ ë‹¤ë¥¼ ë°” ì—†ì´ ë‚˜íƒ€ë‚œë‹¤.
- **MuHPC**ì˜ ê²°ê³¼, **person-level GT**ì™€ ë§¤ìš° ë¹„ìŠ·í•˜ê²Œ ë‚˜ì˜¨ë‹¤.
    - ë³¸ ë°©ë²•ì„ í†µí•´ co-occurrence, ì¦‰ interactionì„ ìë™ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì°¾ì„ ìˆ˜ ìˆë‹¤.

<br>

---

## 6. Conclusion

1. ìƒˆë¡œìš´ multi-modal person-clustering ë°©ë²•ì¸ **MuHPC**ë¥¼ ì œì•ˆí•˜ì˜€ë‹¤.
2. í•´ë‹¹ ë¶„ì•¼ì—ì„œ ê°€ì¥ í¬ê³  ë‹¤ì–‘í•œ datasetì¸ **VPCD**ë¥¼ ì œì‘í•˜ì˜€ë‹¤.
3. MuHPCì™€ VPCDë¥¼ í†µí•´ **í° ì„±ëŠ¥ í–¥ìƒ**ì„ ì´ëŒì–´ëƒˆë‹¤.
4. **ê° characterì˜ appearance**ì™€ **co-occurrence**ë¥¼ ìë™ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.

<br>

---

## 7. Additional Materials
> ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech ìµœì¢… í”„ë¡œì íŠ¸ì— ì ìš©í•˜ê¸° ìœ„í•´ ì°¾ì•„ë´¤ì—ˆë˜ ì—¬ëŸ¬ ìë£Œë“¤

- **ì €ìì˜ ì„¤ëª… ì˜ìƒ**: [Face, Body, Voice: Video Person-Clustering with Multiple Modalities - Spotlight](https://youtu.be/ho58dTFO9kg)
    
- **ICCV 2021 open access**: [ICCV 2021 Open Access Repository](https://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Brown_Face_Body_Voice_Video_Person-Clustering_With_Multiple_Modalities_ICCVW_2021_paper.html)

- **Video Person-Clustering Dataset**: [VPCD](https://www.robots.ox.ac.uk/~vgg/data/Video_Person_Clustering/)

- **GitHub**: [Video_Person_Clustering](https://github.com/Andrew-Brown1/Video_Person_Clustering) (ì½”ë“œëŠ” ì»¤ë°ì‘¨ì´ë¼ í•œë‹¤...)

-   <details>
    <summary>VPCD README.txt</summary>
    <div markdown="1">

    ```
    This README details the contents of the VPCD dataset tar.gz

    For any questions, email Andrew Brown: abrown@robots.ox.ac.uk

    If you find this dataset useful, please consider citing: 

    @misc{Brown21c,
        author       = "Andrew Brown and Vicky Kalogeiton and Andrew Zisserman ",
        title        = "Face, Body, Voice: Video Person-Clustering with Multiple Modalities",
        year={2021},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
    }

    VPCD consists of person-tracks, and voice-tracks. Each person-track consists of either a face-track, or a body-track, or both. A face-track consists of linked face-detections in time. A body-track consists of linked body-detections in time. The entire audio-track of each program set is manually diarised into individual voice-tracks, which are all identity labelled. 

    Assigning voice-tracks to person-tracks: Voice-tracks can be linked to person-tracks by looking at temporal intersection between voice-tracks and person-tracks that are labelled with the same identity. Most of the time, for each voice-track there will be a co-occuring person-track with the same-ground truth label. Occasionally a voice-track corresponds to someone who is off camera. In these cases there will be no co-occurring person-track. 

    VPCD.zip contains a pickle file for each of the program sets included in VPCD. Each pickle file stores a python dictionary data structure. Each key corresponds to each episode of the program set. Each value stores another dictionary with the following keys:

    1) â€˜faceâ€™

    This is a python dictionary, where each key is a person-track identifier, and the values are the face-track coordinates of the face-track for this person-track. The face-track coordinates are a numpy array where each row details a face detection in the track and is structured as [frame number, x1, y1, x2, y2].

    2) â€˜face_featsâ€™

    This is a python dictionary, where each key is a person-track identifier, and the values are the 256D identity-discriminating features for the face-tracks, extracted from a SE-NET50 trained on VGGFace2 and MS1M (https://arxiv.org/abs/1710.08092). The features are stored in a numpy array where each row corresponds to the feature for that given face-detection in the face-track.

    3) â€˜GTâ€™

    This is a python dictionary, where each key is a person-track identifier, and the values are the ground-truth labels.

    4) 'body'

    This is a python dictionary, where each key is a person-track identifier, and the values are the body-track coordinates of the body-track for this person-track. The body-track coordinates are a numpy array where each row details a body detection in the track and is structured as [frame number, x1, y1, x2, y2].

    5) 'body_feats'

    This is a python dictionary, where each key is a person-track identifier, and the values are the 256D identity-discriminating features for the body-tracks, extracted from a ResNet50 trained on Cast-Search in Movies (https://arxiv.org/abs/1807.10510). The features are stored in a numpy array where each row corresponds to the feature for that given body-detection in the body-track.

    6) 'back'

    This is a python dictionary, where each key is a person-track identifier, and the values are the body-track coordinates of the body-track for this person-track. The person-tracks here are those where the face is not visible, hence there is no matching face-track, and hence the 'back' label. The body-track coordinates are a numpy array where each row details a body detection in the track and is structured as [frame number, x1, y1, x2, y2].

    7) 'back_feats'

    Same as 'body_feats', but for the person-tracks with no matching faces.

    8) 'back_GT'

    same as 'GT', but for the person-tracks with no matching faces.

    9) 'Voice'

    This is a python dictionary containing the annotations for the audio track diarisation. Each key is a ground truth label (the same as those in â€˜GTâ€™). Each value is a dictionary which details all of the voice-tracks in the episode that are labelled as being spoken by this ground truth label (identity), with the following keys:

        9i) 'starts' - This is a list, detailing the start times (in seconds) of each of the voice-tracks.
        9ii) 'ends' - This is a list, detailing the end times (in seconds) of each of the voice-tracks.
        9iii) 'frames' - This is a list, detailing the frames that (in seconds) each of the voice-tracks are spoken in.
        9iv) 'features' - The 512D identity-discriminating features extracted from a Thick-ResNet34 trained on VoxCeleb2 (https://arxiv.org/pdf/1806.05622.pdf).

    In some program sets, there is a key in the 'voice' dictionary labelled as â€œlaughterâ€. This label is given to voice-tracks corresponding to laughter (either live studio audience, or a characterâ€™s laughter).
    ```

    </div>
    </details>
    