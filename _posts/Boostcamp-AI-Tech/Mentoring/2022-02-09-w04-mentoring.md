---
title: "[Week 04] 멘토링"
date: 2022-02-09 23:00:00 +0900
categories: [부스트캠프 AI Tech 3기, Mentoring]
tags: [부스트캠프AITech, mentoring, week04, transformer, ResNet, SGD, Adam, transfer-learning]     # TAG names should always be lowercase
image: 
  path: /assets/img/posts/Boostcamp-AI-Tech/boostcamp_ai_tech_title.png
math: true
---

## **수업 관련**
1. **MHA의 각 attention head가 각기 다른 요소에 대해 attention을 갖는 원리가 무엇인지 궁금하다.**
    - 따로 원리가 있다기보다는, 실제로 해보니까 잘 된 것이라고 생각하면 된다.
    - AI 분야에서는 이런 느낌인 게 은근히 많다.
2. **Positional encoding**
    - 이것 또한 큰 의미가 없다. 그냥 여러 position을 다르게 encoding 하기 위한 것이라고 생각하면 된다.
    - 이것 자체도 학습이 가능하다. (trainable)
    - transformer는 input 길이가 고정된 값이다. 대신 엄청 크다.
3. **ResNet에서 skip connection으로 residual만 학습시키는 것의 원리**
    - 연구자들이 ‘잘 될 것 같아서' 시도해 본 것이 실제로 잘 된 케이스이다.
    - 후속 논문들에서 왜 잘 되는지에 대한 연구가 이뤄지기도 한다.
    - ResNet 계열의 모델은 범용성이 좋다.
        - ResNet, ResNext, Res2Net, ResNest
4. **MixUp, CutMix 등의 data augmentation 방법론**
    - 각 방법이 효과적인 문제의 유형 정도만 알고 있어도 충분하다.
5. **SGD와 Adam**
    - **SGD** - 초기 `lr` 값에 너무 민감하다.
        - 최적의 `lr` 값을 찾는 것이 관건이다.
        - local minimum에 도달하더라도 flatter 한 것에 도달하게 된다. train과 dest 차이가 적으므로 test accr가 더 좋다.
    - **Adam** - `lr`을 덜 섬세하게 잡아도 웬만큼 good
        - `adaptive` ⇒ 수렴속도 faster
        - 하지만 여전히 최적의 `lr` 값을 찾는 것은 중요하다. (hyperparameter tuning)
    - 두 방법마다 잘 되는 task가 다르다. 실제로는 여러 `lr`과 함께 Adam, SGD 등의 방법을 모두 조합해서 해본다.
    - 스케줄러도 매우 중요하다.
        - 초반에는 `lr`을 크게 주어야 local minimum에 빠질 위험을 줄일 수 있다.
    - 경험적으로 해야 한다. (이미 잘 알려진 정보들을 이용할 줄 알아야 한다.)

<br>

## **심화 과제 관련**

> transfer learning 심화 과제에서 이미지 셋을 쓴 경우와 안 쓴 경우의 차이가 컸는가?  
> ⇒ 차이가 크지 않았다. (transfer learning의 효과가 크지 않았다.)

- **transfer learning의 효과가 크지 않았던 이유**
    1. 기존 학습된 데이터(source data)와 추가 데이터(target data)의 **domain**이 너무 달랐다.
    2. target data가 충분히 많았다.
- **transfer learning이 효과적이려면?**
    1. src data와 target data의 domain이 비슷해야 한다.
    2. target data가 적을수록 효과적이다.
- **transfer learning에 대한 이야기**
    - weight의 관점에서 보면, training을 많이 시키면 시킬수록 원래 지식을 잃어버리게 된다.
    - 언제 fine-tuning을 멈춰야 하는지, freezing이 중요하다.
- 현업에서는 모델을 scratch부터 만들기보다는, 오히려 transfer learning을 많이 사용한다.
    - 경험적으로 접근하는 것이 좋은 것 같다.
    - hyperparameter tuning 시, 아직은 Ray와 같은 툴보다는 사람이 더 나은 것 같다.