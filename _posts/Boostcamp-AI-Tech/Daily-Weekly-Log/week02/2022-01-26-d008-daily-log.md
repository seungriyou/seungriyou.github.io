---
title: "[Week 02 / Day 008] 학습 기록"
date: 2022-01-26 23:50:00 +0900
categories: [부스트캠프 AI Tech 3기, Daily & Weekly Log]
tags: [부스트캠프AITech, daily-log, week02, peer-session, pytorch, lv1-u-stage]     # TAG names should always be lowercase
image: 
  path: /assets/img/posts/Boostcamp-AI-Tech/boostcamp_ai_tech_title.png
math: true
---
## **📝 오늘 한 일**
- 데일리 스크럼
- 멘토링
- 강의 수강 및 퀴즈
    - [PyTorch 6강]
- 기본 과제 1 완료
- 기본 과제 2
- 코딩테스트 대비

<br>

## **👥 피어세션 요약**
- **Further Question (PyTorch 4강)**
    1. **1 epoch에서 이뤄지는 모델 학습 과정을 정리해보고 성능을 올리기 위해서 어떤 부분을 먼저 고려하면 좋을지 같이 논의해보세요.**
        - loss를 바꾼다. (GAN에서 확실히 많이 쓰인다.)
        - model을 바꿀 수도 있다. (문제의 의도는 아닌 것 같다.)
        - epoch 마다 data shuffle을 해본다. (실제 서비스에서는 괜찮을 것 같은데, 대회에서는 잘 모르겠다.)
    2. **`optimizer.zero_grad()`를 안하면 어떤 일이 일어날지 그리고 매 batch step마다 항상 필요한지 같이 논의해보세요.**
        - step마다 초기화를 하지 않으면 부호의 문제 때문에 gradient의 방향, 즉 학습의 방향이 잘못 될 수 있다.
        - `no_grad()`, `eval()`도 주의해서 기억하면 좋다.
- **강의에서 언급된 stepping freezing(?)의 정확한 명칭은?**
    - adaptive fine-tuning, SpotTune 인 것으로 보인다. ([https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_SpotTune_Transfer_Learning_Through_Adaptive_Fine-Tuning_CVPR_2019_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_SpotTune_Transfer_Learning_Through_Adaptive_Fine-Tuning_CVPR_2019_paper.pdf))

<br>

## **🏫 멘토링 요약**
### **Gather**
- 원하는 index의 값을 가져오기 위함이다.
- matrix가 sparse한 경우에, training 내부에서 들어오는 data의 shpae를 맞춰주는 것이 병렬화에 편하기 때문에 사용한다.
- 모델의 내부보다 후처리 할 때 더 많이 쓰는 것 같다.
    
    ⇒ 하지만 대체제가 존재한다. (ex. index masking 하면 비슷한 효과)
    

### **Q.backward(gradient=...) 어디에서 쓸까?**
- loss에 backward를 하면 모든 param 마다 gradient가 달린다.
- 여기에서 gradient를 초기화하지 않고 backward 한 번 더 하면 학습의 방향이 잘못 될 수 있다.  
    ⇒ `zero_grad()` 안하면 대참사가 일어날 수 있다.
    
- **loss**는 vector가 아니고 **scalar** 값이다.
    ⇒ `loss.backward()` 함수는 scalar 값에 backward가 적용되므로 오류가 발생하지 않는다.
    
- 하지만, `Q.backward()` **vector/tensor**에 backward를 적용하면 오류가 발생한다.
    - loss를 scalar 값이라고 정해놓았기 때문인 것 같다.
    - 따라서 scalar인 성분만 떼어서 `Q[0].backward()` 로 backward를 적용하면 오류가 발생하지 않는다.
- **vector/tensor**인 경우, scalar 값이 아니기 때문에 최종 loss로 생각하지 않고, backprop 중간의 값이라고 생각한다. 따라서 chain rule에 의해 앞에서 값이 전달되어 온다는 것을 나타내기 위해 `Q.backward(gradient=external_gradient)`로 gradient를 지정해 주는 것 같다.
- 모델이 커서 backprop 중 중간에서 멈춘 다음 다시 이어서 계산할 때나 쓰지 않을까 싶다.

### **loss에 대해서**
> **`loss = loss1 + loss2`일 때, 다음의 두 코드의 결과는?** 
>   **⇒ 같다.**
> 1. `loss.backward()`
> 2. `loss1.backward()`, `loss2.backward()`

  > **`epoch` 내 과정**
  > 1. grad 초기화
  > 2. param에 grad 쌓임
  > 3. optim
  > 4. param update

- **gradient clipping**
    - gradient의 norm(크기)을 어느정도 이하로 제한해두기 위함이다.
    - loss backprop 시 gradient가 쌓여 매우 커질 수 있기 때문이다.
- **freeze**
    - freeze 시키고 싶은 것에만 freeze 할 수 있다.
    - freeze 대신, freeze 시키고 싶은 것의 grad를 0으로 설정하면 update가 되지 않아 같은 동작을 하게 된다.
        
        ```python
        loss.backward()
        a.grad = 0.0
        optim.step()
        ```
        
    - 이는 $a = a-\text{lr}*\text{grad}$ 이기 때문이다.
    - 다음과 같이 코드를 작성하면 0.1 정도만 학습되게끔 지정이 가능하다.
        
        ```python
        a.grad *= 0.1
        ```
        
    
    ⇒ 이렇게 customizing 할 수 있다는 것이 PyTorch의 장점이다.
    
### **기타**
- `prefetch` - loader 성능이 매우 좋아진다.
- TensorBoard
- TensorFlow는 배포 시에 좋다.

<br>

## **📚 과제 내용 정리**
따로 Notion에 정리 중이다.

<br>

## **🐾 일일 회고**
이번주는 과제의 데드라인이 목요일 오전이다. 하지만 처음부터 강의와 과제의 정리에 집중하다보니 시간 관리를 잘 못한 것 같다. 이제야 기본 과제 1을 마쳤고, 기본 과제 2는 오늘 밤을 새서 해야할 것 같다. 😭 다음부터는 일정과 시간을 미리 확인하여 이런 일이 없도록 해야할 것 같다.

그리고 오늘부터는 피어세션에서 강의에 달려있는 Further Question에 대해서도 이야기를 나누기로 하였다. (확실히 혼자 복습할 때 해야겠다고 생각만 하면 계속 미루게 된다.) 아직은 모더레이터로서는 너무나도 미숙하지만, 팀원분들의 도움을 받아 피어세션 시간이 점점 자리를 잡아가는 것 같다! 또한, 오늘은 멘토링이 있기도 했는데, 정말 유익했다. 피어세션에서 완벽히 해결하지 못했던 PyTorch의 backward 함수에 대한 질문을 멘토링 시간에도 다루었기 때문이다. 멘토님께서 실제 코드를 써내려가면서 바로 결과를 보여주셨고 덕분에 쉽게 이해를 할 수 있었다. 다음 멘토링부터는 논문 리딩도 병행하기로 하였다!

아직까지는 겨우겨우 따라가는 것만으로도 벅찬 느낌이 들지만 하루하루 얻어가는 것이 많아 뿌듯하다. 우선은 제발 기본 과제 2를 완성하고 제출하자..

<br>

## **🚀 내일 할 일**
- 기본 과제 1, 2 제출하기
- 강의 수강 & 퀴즈
    - [PyTorch 7강]
    - [PyTorch 8강]
- 마스터 클래스